# Example configuration for LSHDataModule
# This file demonstrates all available configuration options

# Data configuration
data:
  # Root directory containing processed, hive-partitioned data
  # Expected structure: base_path/{train,val,test}/REGION_NAME=.../data_type=.../...
  base_path: "/Users/cooper/Desktop/CARAVAN_CLEAN"
  
  # ========== BASIN SPECIFICATION OPTIONS ==========
  # You must specify basins using ONE of these three methods:
  
  # Option 1: Region-based (gets all basins from specified region(s))
  # Can be a single region name (e.g., "camels") or a list (e.g., ["camels", "hysets"])
  region: "tajikkyrgyz"
  
  # Option 2: Explicit basin list (overrides region if both are specified)
  # Useful for small, specific basin selections
  # gauge_ids: ["basin1", "basin2", "basin3"]
  
  # Option 3: External file with basin IDs (takes priority over both region and gauge_ids)
  # Useful for large lists (thousands of basins)
  # File should contain one basin ID per line
  # gauge_ids_file: "/path/to/basin_ids.txt"
  
  # Optional: Path to fitted preprocessing pipeline for inverse transforms
  # Used to transform predictions back to original scale
  pipeline_path: ""  # Optional

# Feature configuration
features:
  # Forcing features: Time-varying input features
  # These should match column names in your processed data
  forcing:
    - "streamflow"
    - "total_precipitation_sum"
  
  # Static features: Time-invariant attributes
  # These should match column names in your static attributes data
  static:
    - "area"
  
  # Future features: Known future covariates (optional)
  # Must be a subset of forcing features
  # These are features known at prediction time (e.g., weather forecasts)
  future: []  # Empty list means no future features
  # Example with future features:
  # future:
  #   - "total_precipitation_sum"
  
  # Target variable: The feature to predict
  # Must be one of the forcing features
  target: "streamflow"

# Sequence configuration
sequence:
  # Length of input sequences (lookback window)
  input_length: 365
  
  # Length of output sequences (prediction horizon)
  output_length: 10

# Data preparation configuration
# Settings that control how data is prepared for the model
data_preparation:
  # Whether the model is autoregressive
  # true: Target is included in input features (for next-step prediction)
  # false: Target is excluded from input features (for direct prediction)
  is_autoregressive: true
  
  # Whether to include date information
  # If true, includes input_end_date timestamp in samples
  include_dates: true

# Model configuration
# Specifies which model to use and its hyperparameters
model:
  # Model architecture to use
  # Available: tide, ealstm, tsmixer, tft, naive_last_value
  type: "tide"
  
  # Optional: Path to external hyperparameter file
  # If specified, loads model hyperparameters from this file
  # config_file: "configs/models/tide_best.yaml"
  
  # Optional: Override specific hyperparameters
  # These take precedence over values in config_file
  # overrides:
  #   hidden_size: 128
  #   dropout: 0.1
  #   learning_rate: 0.001

# DataLoader configuration
dataloader:
  # Batch size for training/validation/testing
  batch_size: 1024
  
  # Number of worker processes for data loading
  # 0 means data is loaded in the main process
  num_workers: 4
  
  # Whether to pin memory for GPU transfer
  # Set to true when using GPU for faster data transfer
  pin_memory: true
  
  # Whether to keep workers alive between epochs
  # Only applies when num_workers > 0
  persistent_workers: true
  
  # Whether to shuffle training data
  # Validation and test data are never shuffled
  shuffle_train: true

# ========== ALTERNATIVE CONFIGURATION EXAMPLES ==========

# Example 1: Minimal configuration with region
# ---
# data:
#   base_path: "/data/processed"
#   region: "camels"
# 
# features:
#   forcing: ["streamflow", "precipitation", "temperature"]
#   static: ["area", "elevation"]
#   target: "streamflow"
# 
# sequence:
#   input_length: 365
#   output_length: 1
# 
# data_preparation:
#   is_autoregressive: true
# 
# model:
#   type: "tide"
# 
# dataloader:
#   batch_size: 32

# Example 2: Using explicit gauge IDs (no region needed)
# ---
# data:
#   base_path: "/data/processed"
#   gauge_ids: ["01013500", "01030500", "01031500", "01047000", "01052500"]
# 
# features:
#   forcing: ["streamflow", "precipitation", "temperature"]
#   static: ["area", "elevation"]
#   target: "streamflow"
# 
# sequence:
#   input_length: 365
#   output_length: 1
# 
# data_preparation:
#   is_autoregressive: true
# 
# model:
#   type: "tide"
# 
# dataloader:
#   batch_size: 32

# Example 3: Using external file for large basin lists
# ---
# data:
#   base_path: "/data/processed"
#   gauge_ids_file: "/experiments/basin_subset_5000.txt"

# features:
#   forcing: ["streamflow", "precipitation", "temperature"]
#   static: ["area", "elevation"]
#   target: "streamflow"

# sequence:
#   input_length: 365
#   output_length: 1

# data_preparation:
#   is_autoregressive: true

# model:
#   type: "tsmixer"
#   config_file: "configs/models/tsmixer_tuned.yaml"
#   overrides:
#     learning_rate: 0.0001

# dataloader:
#   batch_size: 32